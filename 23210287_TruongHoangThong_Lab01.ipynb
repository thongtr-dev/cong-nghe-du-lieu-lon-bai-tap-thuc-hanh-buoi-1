{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f9c014",
   "metadata": {},
   "source": [
    "# Bài tập Thực hành Buổi 01\n",
    "\n",
    "**Họ và tên:** Trương Hoàng Thông  \n",
    "**MSSV:** 23210287\n",
    "\n",
    "---\n",
    "**Môn học:** Công Nghệ Phân Tích Dữ Liệu Lớn\n",
    "**Giảng viên:** ThS. Nguyễn Hồ Duy Trí\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007eb29e",
   "metadata": {},
   "source": [
    "## 1. Import thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91df4f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054f5d9d",
   "metadata": {},
   "source": [
    "## 2. Tạo dãy số thực ngẫu nhiên và DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cfc4206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/01 02:01:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day so thuc: [22.648612431181046, 82.2494911779238, 78.05666092835338, 86.21396904999486, 98.75418444171784, 25.202660219413055, 47.13493768382394, 13.688899166855073, 97.18933163152694, 18.280502495569284]\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Lab01_PySpark\").getOrCreate()\n",
    "\n",
    "numbers = [random.uniform(0, 100) for _ in range(10)]\n",
    "print(f\"Day so thuc: {numbers}\")\n",
    "df = spark.createDataFrame([(float(x),) for x in numbers], [\"value\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d0a7cf",
   "metadata": {},
   "source": [
    "## 3. Tính trung bình cộng và trung bình nhân"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c41ca2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trung binh cong: 56.94192492263593\n",
      "Trung binh nhan: 45.221842245000644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Tinh trung binh cong\n",
    "trung_binh_cong = df.agg(F.avg(\"value\")).collect()[0][0]\n",
    "\n",
    "# Tinh trung binh nhan, chi tinh neu tat ca cac so deu duong\n",
    "if all(x > 0 for x in numbers):\n",
    "    log_mean = df.agg(F.avg(F.log(\"value\"))).collect()[0][0]\n",
    "    trung_binh_nhan = math.exp(log_mean)\n",
    "else:\n",
    "    trung_binh_nhan = None\n",
    "\n",
    "print(f\"Trung binh cong: {trung_binh_cong}\")\n",
    "print(f\"Trung binh nhan: {trung_binh_nhan}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51749a88",
   "metadata": {},
   "source": [
    "## 4. Tính phương sai và độ lệch chuẩn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccdf580b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phuong sai: 1217.3772635541438\n",
      "Do lech chuan: 34.89093383035404\n"
     ]
    }
   ],
   "source": [
    "# Tinh phuong sai\n",
    "phuong_sai = df.agg(F.variance(\"value\")).collect()[0][0]\n",
    "print(f\"Phuong sai: {phuong_sai}\")\n",
    "\n",
    "# Tinh do lech chuan\n",
    "do_lech_chuan = df.agg(F.stddev(\"value\")).collect()[0][0]\n",
    "print(f\"Do lech chuan: {do_lech_chuan}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63fb1f4",
   "metadata": {},
   "source": [
    "## 5. Đọc dữ liệu phân số từ file và tính tổng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e2b10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tong day phan so: 151/40\n"
     ]
    }
   ],
   "source": [
    "# Euclid, tim uoc so chung lon nhat\n",
    "def gcd(a, b):\n",
    "    while b != 0:\n",
    "        a, b = b, a % b\n",
    "    return a\n",
    "\n",
    "rdd = spark.sparkContext.textFile(\"data_phanso.txt\")\n",
    "day_phan_so = rdd.map(lambda line: tuple(map(int, line.strip().split(\"/\"))))\n",
    "tu_mau = day_phan_so.collect()\n",
    "\n",
    "mau_so_chung = 1\n",
    "for _, mau in tu_mau:\n",
    "    mau_so_chung *= mau\n",
    "\n",
    "tong_tu_so = 0\n",
    "for tu, mau in tu_mau:\n",
    "    tong_tu_so += tu * (mau_so_chung // mau)\n",
    "\n",
    "ucln = gcd(tong_tu_so, mau_so_chung)\n",
    "tong_tu_so //= ucln\n",
    "mau_so_chung //= ucln\n",
    "\n",
    "print(f\"Tong day phan so: {tong_tu_so}/{mau_so_chung}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "23210287",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
